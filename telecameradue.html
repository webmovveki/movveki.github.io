<!DOCTYPE html>
<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta charset="UTF-8" />
    <link rel="shortcut icon" type="image/x-icon" href="/static/favicon.ico" />
    <link href="stile_progetto.css" rel="stylesheet" type="text/css" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Space+Grotesk:wght@300;400;500;600;700&display=swap"
      rel="stylesheet"
    />
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.4.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.1/dist/face-landmarks-detection.js"></script>
    <title>Movvek√¨</title>
  </head>

  <body>
    <nav id="main-nav" class="responsive">
      <div id="logo">
        <a href="./index.html"
          ><img src="Immagini/logo_movveki.png" height="70px" width="73,83px" />
        </a>
      </div>

      <div>
        <button class="button-nav">
          <a href="./home.html"><h3>HOME</h3></a>
        </button>
        <button class="button-nav">
          <a href="./chisiamo.html"> <h3>CHI SIAMO</h3></a>
        </button>
      </div>
    </nav>

    <main id="telecamera">
      <canvas id="output"></canvas>

      <h1 id="status"></h1>

      <div>
        <p id="test-header" style="display: block">Posizionati al centro!</p>
      </div>

      <div id="telecameraImg" class="img-telecamera">
        <img
          id="immagine"
          src="Immagini/Icona_omino.png"
          height="498"
          width="670"
        />
      </div>

      <div>
        <button class="button-test" onclick="avviaCamera()">
          <h3>Scatta</h3>
        </button>

        <button class="button-test">
          <a href="#"> <h3>Riprova</h3></a>
        </button>
      </div>
      <video id="webcam" playsinline style="width: auto; height: auto"></video>
    </main>

    <script>
      function setText(text) {
        document.getElementById("status").innerText = text;
      }

      function drawLine(ctx, x1, y1, x2, y2) {
        ctx.beginPath();
        ctx.moveTo(x1, y1);
        ctx.lineTo(x2, y2);
        ctx.stroke();
      }

      async function setupWebcam() {
        return new Promise((resolve, reject) => {
          const constraints = { video: { width: 640, height: 480 } };
          const webcamElement = document.getElementById("webcam");
          navigator.mediaDevices
            .getUserMedia(constraints)
            .then(function (mediaStream) {
              console.log("we have the mediaStream", mediaStream);
              webcamElement.srcObject = mediaStream;
              webcamElement.addEventListener("loadeddata", resolve, false);
            })
            .catch(function (err) {
              console.log(err.name + ": " + err.message);
            });
        });
      }

      const emotions = [
        "arrabbiato",
        "disgustato",
        "impaurito",
        "felice",
        "neutrale",
        "triste",
        "sorpreso",
      ];
      let emotionModel = null;

      let output = null;
      let model = null;

      async function predictEmotion(points) {
        let result = tf.tidy(() => {
          const xs = tf.stack([tf.tensor1d(points)]);
          return emotionModel.predict(xs);
        });
        let prediction = await result.data();
        result.dispose();
        // Get the index of the maximum value
        let id = prediction.indexOf(Math.max(...prediction));
        return emotions[id];
      }

      async function trackFace() {
        const stream = document.querySelector("video");
        const faces = await model.estimateFaces({
          input: stream,
          returnTensors: false,
          flipHorizontal: false,
        });
        output.drawImage(
          stream,
          0,
          0,
          stream.width,
          stream.height,
          0,
          0,
          stream.width,
          stream.height
        );

        requestAnimationFrame(trackFace);
      }

      const avviaCamera = async () => {
        const stream = document.querySelector("video");
        const faces = await model.estimateFaces({
          input: stream,
          returnTensors: false,
          flipHorizontal: false,
        });
        output.drawImage(
          stream,
          0,
          0,
          stream.width,
          stream.height,
          0,
          0,
          stream.width,
          stream.height
        );

        let points = null;
        faces.forEach((face) => {
          // Draw the bounding box
          const x1 = face.boundingBox.topLeft[0];
          const y1 = face.boundingBox.topLeft[1];
          const x2 = face.boundingBox.bottomRight[0];
          const y2 = face.boundingBox.bottomRight[1];
          const bWidth = x2 - x1;
          const bHeight = y2 - y1;
          drawLine(output, x1, y1, x2, y1);
          drawLine(output, x2, y1, x2, y2);
          drawLine(output, x1, y2, x2, y2);
          drawLine(output, x1, y1, x1, y2);

          // Add just the nose, cheeks, eyes, eyebrows & mouth
          const features = [
            "noseTip",
            "leftCheek",
            "rightCheek",
            "leftEyeLower1",
            "leftEyeUpper1",
            "rightEyeLower1",
            "rightEyeUpper1",
            "leftEyebrowLower", //"leftEyebrowUpper",
            "rightEyebrowLower", //"rightEyebrowUpper",
            "lipsLowerInner", //"lipsLowerOuter",
            "lipsUpperInner", //"lipsUpperOuter",
          ];
          points = [];
          features.forEach((feature) => {
            face.annotations[feature].forEach((x) => {
              points.push((x[0] - x1) / bWidth);
              points.push((x[1] - y1) / bHeight);
            });
          });
        });

        if (points) {
          let emotion = await predictEmotion(points);
          window.location.replace("emozione.html?e=" + emotion);
          setText(`Detected: ${emotion}`);
        } else {
          setText("No Face");
        }
      };

      (async () => {
        console.log("Setting up webcam");
        await setupWebcam();
        console.log("webcam set up");
        document
          .getElementById("telecameraImg")
          .parentNode.removeChild(document.getElementById("telecameraImg"));
        const video = document.getElementById("webcam");
        video.play();
        let videoWidth = video.videoWidth;
        let videoHeight = video.videoHeight;
        video.width = videoWidth;
        video.height = videoHeight;

        let canvas = document.getElementById("output");
        canvas.width = video.width;
        canvas.height = video.height;

        output = canvas.getContext("2d");
        output.translate(canvas.width, 0);
        output.scale(-1, 1); // Mirror cam
        output.fillStyle = "#fdffb6";
        output.strokeStyle = "#fdffb6";
        output.lineWidth = 2;

        // Load Face Landmarks Detection
        model = await faceLandmarksDetection.load(
          faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
        );
        console.log("model loaded", model);
        // Load Emotion Detection
        emotionModel = await tf.loadLayersModel("web/model/facemo.json");
        console.log("starting trackface()");
        trackFace();
      })();
    </script>
  </body>
  <!-- <script src="face-emotion.js"></script> -->
  <script src="./script.js"></script>
  <script>
    cambia();
  </script>
</html>
